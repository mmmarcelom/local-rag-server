#!/usr/bin/env python3
"""
Script de diagn√≥stico para verificar o status do Ollama
"""
import asyncio
import httpx
import subprocess
import sys
from config import OLLAMA_MODEL, OLLAMA_HOST, OLLAMA_PORT, OLLAMA_URL

async def check_ollama_installation():
    """Verifica se o Ollama est√° instalado"""
    print("üîç Verificando instala√ß√£o do Ollama...")
    
    try:
        result = subprocess.run(["ollama", "--version"], 
                              capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print(f"‚úÖ Ollama instalado: {result.stdout.strip()}")
            return True
        else:
            print("‚ùå Ollama n√£o est√° instalado ou n√£o est√° no PATH")
            return False
    except FileNotFoundError:
        print("‚ùå Ollama n√£o encontrado no sistema")
        print("üí° Instale em: https://ollama.ai")
        return False
    except Exception as e:
        print(f"‚ùå Erro ao verificar instala√ß√£o: {e}")
        return False

async def check_ollama_service():
    """Verifica se o servi√ßo Ollama est√° rodando"""
    print(f"üîç Verificando se Ollama est√° rodando em {OLLAMA_URL}...")
    
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get(f"{OLLAMA_URL}/api/tags")
            if response.status_code == 200:
                print("‚úÖ Ollama est√° respondendo!")
                return True
            else:
                print(f"‚ùå Ollama respondeu com status {response.status_code}")
                return False
    except Exception as e:
        print(f"‚ùå Ollama n√£o est√° respondendo: {e}")
        print("üí° Inicie o Ollama com: ollama serve")
        return False

async def check_models():
    """Verifica modelos dispon√≠veis"""
    print("üîç Verificando modelos dispon√≠veis...")
    
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(f"{OLLAMA_URL}/api/tags")
            models_data = response.json()
            
            available_models = [model["name"] for model in models_data.get("models", [])]
            print(f"üìã Modelos dispon√≠veis: {available_models}")
            
            if not available_models:
                print("‚ö†Ô∏è Nenhum modelo encontrado")
                print("üí° Baixe um modelo: ollama pull llama3.2")
                return False
            
            if OLLAMA_MODEL in available_models:
                print(f"‚úÖ Modelo {OLLAMA_MODEL} encontrado!")
                return True
            else:
                print(f"‚ùå Modelo {OLLAMA_MODEL} n√£o encontrado")
                print(f"üí° Baixe o modelo: ollama pull {OLLAMA_MODEL}")
                return False
                
    except Exception as e:
        print(f"‚ùå Erro ao verificar modelos: {e}")
        return False

async def test_model_generation():
    """Testa gera√ß√£o de resposta"""
    print("üîç Testando gera√ß√£o de resposta...")
    
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{OLLAMA_URL}/api/chat",
                json={
                    "model": OLLAMA_MODEL,
                    "messages": [{"role": "user", "content": "Responda apenas 'OK'"}],
                    "options": {"temperature": 0.1, "max_tokens": 5}
                }
            )
            
            if response.status_code == 200:
                print("‚úÖ Gera√ß√£o de resposta funcionando!")
                return True
            else:
                print(f"‚ùå Erro na gera√ß√£o: {response.status_code}")
                print(f"Resposta: {response.text}")
                return False
                
    except Exception as e:
        print(f"‚ùå Erro ao testar gera√ß√£o: {e}")
        return False

async def main():
    """Executa todos os diagn√≥sticos"""
    print("üöÄ Diagn√≥stico do Ollama")
    print("=" * 50)
    
    # Verificar instala√ß√£o
    installed = await check_ollama_installation()
    if not installed:
        print("\n‚ùå Ollama n√£o est√° instalado. Instale em: https://ollama.ai")
        return
    
    # Verificar servi√ßo
    service_ok = await check_ollama_service()
    if not service_ok:
        print("\n‚ùå Ollama n√£o est√° rodando. Execute: ollama serve")
        return
    
    # Verificar modelos
    models_ok = await check_models()
    if not models_ok:
        print(f"\n‚ùå Modelo {OLLAMA_MODEL} n√£o encontrado. Execute: ollama pull {OLLAMA_MODEL}")
        return
    
    # Testar gera√ß√£o
    generation_ok = await test_model_generation()
    if not generation_ok:
        print("\n‚ùå Gera√ß√£o de resposta falhou")
        return
    
    print("\nüéâ Ollama est√° funcionando perfeitamente!")
    print("‚úÖ Voc√™ pode iniciar o servidor agora!")

if __name__ == "__main__":
    asyncio.run(main()) 